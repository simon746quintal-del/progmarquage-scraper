# ğŸ¤– GUIDE COMPLET - SCRAPING AUTOMATIQUE PROGMARQUAGE

## ğŸ¯ CE QUE LE SYSTÃˆME FAIT

âœ… **DÃ©tecte automatiquement** les nouveaux projets en Savoie (73), Haute-Savoie (74) et Ain (01)
âœ… **Filtre uniquement** les projets RÃ‰CENTS (en construction ou Ã  venir)
âœ… **Identifie** tout ce qui nÃ©cessite du marquage au sol (parking + intÃ©rieur)
âœ… **Estime automatiquement** la valeur du projet et la taille du parking
âœ… **Ajoute les leads** directement dans votre SaaS
âœ… **Tourne 24/7** automatiquement toutes les 6 heures

---

## ğŸ“Š SOURCES SCRAPÃ‰ES

1. **Le DauphinÃ© LibÃ©rÃ©** â†’ ActualitÃ©s locales, ouvertures commerciales
2. **Google News** â†’ Annonces de projets, presse locale
3. **MarchÃ©s Publics (BOAMP)** â†’ Appels d'offres, gros projets
4. **Sites officiels mairies** â†’ Permis de construire (Ã  venir)

---

## ğŸ” CE QUI EST DÃ‰TECTÃ‰

### Types de projets :
- ğŸ¥– **Commerce** : Boulangerie, supermarchÃ©, restaurant, magasin...
- ğŸ­ **Industrie** : Usine, entrepÃ´t, atelier, plateforme logistique...
- ğŸš— **Services** : Garage, station-service, clinique, banque...
- ğŸ¨ **HÃ©bergement** : HÃ´tel, rÃ©sidence hÃ´teliÃ¨re...
- ğŸ¬ **Loisirs** : CinÃ©ma, bowling, salle de sport...
- ğŸ¢ **Bureaux** : Immeubles, siÃ¨ges sociaux...
- ğŸ˜ï¸ **RÃ©sidentiel** : RÃ©sidences, copropriÃ©tÃ©s (parking collectif)

### Filtres temporels :
- âœ… **EN CONSTRUCTION** actuellement
- âœ… **OUVERTURE PRÃ‰VUE** dans les 3-6 mois
- âœ… **PERMIS RÃ‰CENT** (< 6 mois)
- âœ… **PROJET ANNONCÃ‰** pour 2026-2027
- âŒ **DÃ‰JÃ€ OUVERT** depuis > 2 mois (Ã‰LIMINÃ‰)

---

## ğŸš€ INSTALLATION - 3 OPTIONS

### **OPTION 1 : GitHub Actions (RECOMMANDÃ‰ - 100% GRATUIT & AUTO)**

C'est la solution la plus simple ! GitHub va exÃ©cuter le scraper automatiquement pour vous.

#### Ã‰tapes :

1. **CrÃ©ez un compte GitHub** (si pas dÃ©jÃ  fait) : https://github.com

2. **CrÃ©ez un nouveau repository** :
   - Nom : `progmarquage-scraper`
   - VisibilitÃ© : Private

3. **Uploadez les fichiers** :
   - `progmarquage_scraper.py`
   - `requirements.txt`
   - `.github/workflows/auto-scraper.yml`

4. **Configurez les secrets** :
   - Allez dans `Settings` > `Secrets and variables` > `Actions`
   - Cliquez `New repository secret`
   - Ajoutez :
     - `SUPABASE_URL` = `https://exycahcnbdodqljlcygb.supabase.co`
     - `SUPABASE_KEY` = `votre_clÃ©_supabase`

5. **Activez GitHub Actions** :
   - Allez dans l'onglet `Actions`
   - Activez les workflows

6. **Lancez le premier scraping** :
   - Dans `Actions` > `ProgMarquage Auto Scraper`
   - Cliquez `Run workflow`

âœ… **C'EST TOUT !** Le scraper tournera maintenant automatiquement toutes les 6 heures !

---

### **OPTION 2 : Serveur Cloud Gratuit (Render.com)**

Pour exÃ©cuter le scraper sur un serveur dÃ©diÃ©.

#### Ã‰tapes :

1. **CrÃ©ez un compte** sur https://render.com

2. **CrÃ©ez un nouveau Cron Job** :
   - Type : Cron Job
   - Repository : Votre repo GitHub
   - Build Command : `pip install -r requirements.txt`
   - Command : `python progmarquage_scraper.py`
   - Schedule : `0 */6 * * *` (toutes les 6h)

3. **Ajoutez les variables d'environnement** :
   - `SUPABASE_URL`
   - `SUPABASE_KEY`

âœ… Le scraper tournera automatiquement sur Render !

---

### **OPTION 3 : En local sur votre ordinateur**

Pour tester ou exÃ©cuter manuellement.

#### Ã‰tapes :

1. **Installez Python 3.11** : https://www.python.org/downloads/

2. **Installez les dÃ©pendances** :
```bash
pip install -r requirements.txt
```

3. **Lancez le scraper** :
```bash
python progmarquage_scraper.py
```

4. **Pour automatiser** (Windows) :
   - Utilisez le Planificateur de tÃ¢ches Windows
   - CrÃ©ez une tÃ¢che qui lance le script toutes les 6h

---

## ğŸ“‹ MODIFICATION DU SCRIPT POUR AJOUT AUTO Ã€ SUPABASE

Pour que les leads soient **automatiquement ajoutÃ©s** dans votre SaaS :

### Dans `progmarquage_scraper.py`, ligne 317 :

**AVANT (version test) :**
```python
# Pour l'instant, juste afficher
# En production, dÃ©commenter:
# response = supabase.table('leads').insert(lead).execute()
```

**APRÃˆS (version production) :**
```python
# Ajout automatique dans Supabase
response = supabase.table('leads').insert(lead).execute()
print(f"  âœ… Lead ajoutÃ© Ã  Supabase: {lead['name'][:50]}")
```

âš ï¸ **PROBLÃˆME** : Il faut un `user_id` pour Supabase RLS.

### SOLUTION : CrÃ©er un utilisateur "systÃ¨me"

1. Dans Supabase, crÃ©ez un compte email : `scraper@progmarquage.fr`
2. RÃ©cupÃ©rez son `user_id` dans la table `auth.users`
3. Ajoutez ce `user_id` Ã  tous les leads automatiques

**OU MIEUX** : Modifier la politique RLS pour permettre l'insertion sans user_id pour un service account.

---

## ğŸ”§ AMÃ‰LIORER LE SCRAPING

### Ajouter plus de sources :

1. **Permis de construire officiels** :
   - Sites des mairies
   - Registres publics

2. **RÃ©seaux sociaux** :
   - Facebook (pages de zones commerciales)
   - LinkedIn (annonces d'entreprises)

3. **Sites immobiliers** :
   - SeLoger, LeBonCoin (commerces Ã  louer/vendre)

4. **APIs gouvernementales** :
   - data.gouv.fr
   - API cadastre

---

## ğŸ“§ AJOUTER DES ALERTES EMAIL

Pour recevoir un email quand un lead urgent est dÃ©tectÃ© :

### Installer SendGrid (gratuit 100 emails/jour) :

```bash
pip install sendgrid
```

### Ajouter dans le code :

```python
from sendgrid import SendGridAPIClient
from sendgrid.helpers.mail import Mail

def send_alert_email(lead):
    message = Mail(
        from_email='scraper@progmarquage.fr',
        to_emails='votre-email@progmarquage.fr',
        subject=f'ğŸš¨ LEAD URGENT : {lead["name"]}',
        html_content=f'''
            <h2>Nouveau lead dÃ©tectÃ© !</h2>
            <p><strong>{lead["name"]}</strong></p>
            <p>ğŸ“ {lead["location"]}</p>
            <p>ğŸ’° {lead["estimated_value"]}</p>
            <p>ğŸ…¿ï¸ {lead["parking_size"]}</p>
        '''
    )
    
    sg = SendGridAPIClient('VOTRE_API_KEY_SENDGRID')
    sg.send(message)
```

---

## ğŸ“Š STATISTIQUES & MONITORING

### Voir combien de leads sont scrapÃ©s :

Le script gÃ©nÃ¨re un fichier JSON aprÃ¨s chaque exÃ©cution :
- `leads_progmarquage_20260209_143022.json`

Vous pouvez consulter ces fichiers pour voir tous les leads dÃ©tectÃ©s.

---

## ğŸ› RÃ‰SOLUTION DE PROBLÃˆMES

### Le scraper ne trouve aucun lead :
â¡ï¸ Normal si pas de nouveaux projets annoncÃ©s rÃ©cemment
â¡ï¸ Attendez quelques jours, le scraper continuera Ã  tourner

### Erreur "Rate limit exceeded" :
â¡ï¸ Le site bloque trop de requÃªtes
â¡ï¸ Augmentez le `time.sleep()` entre les requÃªtes

### Les leads ne s'ajoutent pas Ã  Supabase :
â¡ï¸ VÃ©rifiez les permissions RLS
â¡ï¸ VÃ©rifiez que le `user_id` est correct

---

## ğŸ¯ PROCHAINES Ã‰TAPES

1. âœ… **Tester le scraper** manuellement
2. âœ… **DÃ©ployer sur GitHub Actions** pour automatisation
3. âœ… **VÃ©rifier** que les leads apparaissent dans votre SaaS
4. ğŸš€ **Ajouter plus de sources** (permis de construire, etc.)
5. ğŸ“§ **Configurer les alertes email** pour les leads urgents

---

## âœ… CHECKLIST DÃ‰PLOIEMENT

- [ ] Script `progmarquage_scraper.py` crÃ©Ã©
- [ ] DÃ©pendances installÃ©es (`requirements.txt`)
- [ ] Repository GitHub crÃ©Ã©
- [ ] Secrets configurÃ©s dans GitHub
- [ ] Workflow GitHub Actions activÃ©
- [ ] Premier scraping testÃ© manuellement
- [ ] Leads vÃ©rifiÃ©s dans le SaaS
- [ ] Scraping automatique activÃ© (toutes les 6h)

---

## ğŸ‰ FÃ‰LICITATIONS !

Votre systÃ¨me de scraping automatique est maintenant opÃ©rationnel ! Vous allez recevoir automatiquement tous les nouveaux projets nÃ©cessitant du marquage au sol en Savoie, Haute-Savoie et Ain ! ğŸš€ğŸ”¥

---

## ğŸ“ SUPPORT

En cas de problÃ¨me, vÃ©rifiez :
1. Les logs dans GitHub Actions
2. Les fichiers JSON gÃ©nÃ©rÃ©s
3. Les permissions Supabase
