"""
PROGMARQUAGE - SYST√àME DE SCRAPING AUTOMATIQUE
D√©tecte automatiquement les nouveaux projets n√©cessitant du marquage au sol
R√©gions : Savoie (73), Haute-Savoie (74), Ain (01)
"""

import os
import requests
from datetime import datetime, timedelta
from bs4 import BeautifulSoup
import json
import re
from supabase import create_client, Client
import time

# Configuration Supabase
SUPABASE_URL = "https://exycahcnbdodqljlcygb.supabase.co"
SUPABASE_KEY = "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6ImV4eWNhaGNuYmRvZHFsamxjeWdiIiwicm9sZSI6ImFub24iLCJpYXQiOjE3Mzg4NTE2ODUsImV4cCI6MjA1NDQyNzY4NX0.sb_publishable_T0PbuxYkvXzGME4tS9xLCQ_TXs8eO6M"

# Initialiser Supabase
supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

# D√©partements cibl√©s
DEPARTMENTS = ['73', '74', '01']

# Villes principales par d√©partement
CITIES = {
    '73': ['Chamb√©ry', 'Aix-les-Bains', 'Albertville', 'Cognin', 'La Motte-Servolex', 'Bassens', 'Ugine'],
    '74': ['Annecy', 'Annemasse', 'Thonon-les-Bains', 'Cluses', 'Seynod', 'Rumilly', 'Annecy-le-Vieux', 'Cran-Gevrier', 'Sallanches', 'Bonneville', '√âvian-les-Bains'],
    '01': ['Bourg-en-Bresse', 'Oyonnax', 'Bellegarde-sur-Valserine', 'Amb√©rieu-en-Bugey', 'Ferney-Voltaire', 'Gex', 'Thoiry', 'Divonne-les-Bains']
}

# Types de projets n√©cessitant du marquage au sol
PROJECT_TYPES = {
    'commerce': ['boulangerie', 'supermarch√©', 'hypermarch√©', 'magasin', 'commerce', 'boutique', 'restaurant', 'fast-food', 'caf√©', 'pharmacie', 'opticien'],
    'industrie': ['usine', 'atelier', 'entrep√¥t', 'plateforme logistique', 'zone industrielle', 'manufacture', 'production'],
    'services': ['garage', 'concession', 'station-service', 'banque', 'cabinet m√©dical', 'clinique', 'cabinet dentaire', 'salle de sport', 'piscine'],
    'hebergement': ['h√¥tel', 'r√©sidence h√¥teli√®re', 'motel', 'apart-h√¥tel'],
    'loisirs': ['cin√©ma', 'bowling', 'salle de spectacle', 'centre de loisirs'],
    'bureaux': ['bureaux', 'immeuble de bureaux', 'si√®ge social', 'coworking', 'p√©pini√®re d\'entreprises'],
    'residentiel': ['r√©sidence', 'copropri√©t√©', 'programme immobilier', 'logements collectifs', 'r√©sidence √©tudiante', 'r√©sidence senior']
}

# Mots-cl√©s temporels pour identifier les projets R√âCENTS
RECENT_KEYWORDS = [
    'en construction', 'ouverture pr√©vue', 'bient√¥t', 'projet', 'futur', 
    '√† venir', 'permis d√©pos√©', 'chantier', 'travaux en cours', 
    'livraison pr√©vue', 'mise en service', 'prochainement',
    'en cours de construction', 'construction en cours', 'va ouvrir',
    'ouvrira', 'sera inaugur√©', 'annonc√©', 'pr√©vu pour', '2026', '2027'
]

# Mots-cl√©s √† √âVITER (projets trop anciens)
OLD_KEYWORDS = [
    'inaugur√©', 'a ouvert', 'ouvert depuis', 'depuis 2023', 'depuis 2024',
    'f√™te ses', 'd√©j√† op√©rationnel', 'en activit√© depuis', 'c√©l√®bre'
]

class ProgMarquageScraper:
    def __init__(self):
        self.leads_found = []
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })

    def is_recent_project(self, text):
        """V√©rifie si le projet est r√©cent (pas d√©j√† ouvert depuis longtemps)"""
        text_lower = text.lower()
        
        # V√©rifier les mots-cl√©s d'anciennet√© (√† √©viter)
        for old_keyword in OLD_KEYWORDS:
            if old_keyword in text_lower:
                return False
        
        # V√©rifier les mots-cl√©s de r√©cence
        for recent_keyword in RECENT_KEYWORDS:
            if recent_keyword in text_lower:
                return True
        
        # V√©rifier les dates futures (2026, 2027)
        current_year = datetime.now().year
        if str(current_year) in text or str(current_year + 1) in text:
            return True
        
        return False

    def extract_project_type(self, text):
        """Identifie le type de projet"""
        text_lower = text.lower()
        for category, keywords in PROJECT_TYPES.items():
            for keyword in keywords:
                if keyword in text_lower:
                    return keyword.capitalize()
        return "Commerce"

    def estimate_value(self, project_type, text):
        """Estime la valeur du projet en fonction du type"""
        text_lower = text.lower()
        
        # Gros projets
        if any(word in text_lower for word in ['centre commercial', 'hypermarch√©', 'plateforme logistique', 'usine']):
            if any(word in text_lower for word in ['amazon', 'carrefour', 'leclerc', 'auchan']):
                return "35 000 - 50 000 ‚Ç¨"
            return "18 000 - 35 000 ‚Ç¨"
        
        # Projets moyens
        if any(word in text_lower for word in ['supermarch√©', 'garage', 'concession', 'entrep√¥t', 'r√©sidence']):
            return "8 000 - 18 000 ‚Ç¨"
        
        # Petits projets
        return "3 000 - 8 000 ‚Ç¨"

    def estimate_parking_size(self, project_type, text):
        """Estime la taille du parking"""
        text_lower = text.lower()
        
        # Chercher des chiffres de places mentionn√©s
        places_match = re.search(r'(\d+)\s*places?', text_lower)
        if places_match:
            return f"{places_match.group(1)} places"
        
        # Estimation par type
        if any(word in text_lower for word in ['hypermarch√©', 'centre commercial']):
            return "150-300 places"
        if any(word in text_lower for word in ['supermarch√©', 'grande surface']):
            return "80-150 places"
        if any(word in text_lower for word in ['usine', 'entrep√¥t', 'plateforme']):
            return "50-200 places (VL + PL)"
        if any(word in text_lower for word in ['r√©sidence', 'copropri√©t√©']):
            return "30-80 places"
        
        return "15-40 places"

    def scrape_dauphine_libere(self, department, city):
        """Scrape Le Dauphin√© Lib√©r√© pour les actualit√©s locales"""
        print(f"üîç Scraping Le Dauphin√© Lib√©r√© - {city} ({department})...")
        
        try:
            # URLs de recherche
            search_queries = [
                f"{city} ouverture commerce",
                f"{city} construction magasin",
                f"{city} nouveau commerce",
                f"{city} projet immobilier"
            ]
            
            for query in search_queries:
                url = f"https://www.ledauphine.com/search?q={query.replace(' ', '+')}"
                
                try:
                    response = self.session.get(url, timeout=10)
                    if response.status_code == 200:
                        soup = BeautifulSoup(response.content, 'html.parser')
                        articles = soup.find_all('article', limit=5)
                        
                        for article in articles:
                            title = article.find('h2') or article.find('h3')
                            if title:
                                title_text = title.get_text(strip=True)
                                snippet = article.get_text(strip=True)[:500]
                                
                                if self.is_recent_project(snippet):
                                    project_type = self.extract_project_type(snippet)
                                    
                                    lead = {
                                        'name': title_text[:100],
                                        'type': project_type,
                                        'department': department,
                                        'location': f"{city}, {department}",
                                        'source': 'Le Dauphin√© Lib√©r√©',
                                        'notes': snippet[:300],
                                        'estimated_value': self.estimate_value(project_type, snippet),
                                        'parking_size': self.estimate_parking_size(project_type, snippet),
                                        'status': 'new'
                                    }
                                    
                                    self.leads_found.append(lead)
                                    print(f"  ‚úÖ Lead trouv√©: {title_text[:50]}...")
                    
                    time.sleep(2)  # Respecter le serveur
                
                except Exception as e:
                    print(f"  ‚ö†Ô∏è Erreur requ√™te: {e}")
                    continue
        
        except Exception as e:
            print(f"  ‚ùå Erreur scraping Dauphin√©: {e}")

    def scrape_google_news(self, department, city):
        """Scrape Google News pour les actualit√©s locales"""
        print(f"üîç Scraping Google News - {city} ({department})...")
        
        try:
            queries = [
                f"{city} ouverture commerce 2026",
                f"{city} nouveau magasin",
                f"{city} construction usine",
                f"{city} projet commercial"
            ]
            
            for query in queries:
                url = f"https://news.google.com/search?q={query.replace(' ', '+')}&hl=fr&gl=FR&ceid=FR:fr"
                
                try:
                    response = self.session.get(url, timeout=10)
                    if response.status_code == 200:
                        soup = BeautifulSoup(response.content, 'html.parser')
                        articles = soup.find_all('article', limit=5)
                        
                        for article in articles:
                            title = article.find('a')
                            if title:
                                title_text = title.get_text(strip=True)
                                
                                if self.is_recent_project(title_text):
                                    project_type = self.extract_project_type(title_text)
                                    
                                    lead = {
                                        'name': title_text[:100],
                                        'type': project_type,
                                        'department': department,
                                        'location': f"{city}, {department}",
                                        'source': 'Google News',
                                        'notes': f"Projet d√©tect√© via Google News: {title_text}",
                                        'estimated_value': self.estimate_value(project_type, title_text),
                                        'parking_size': self.estimate_parking_size(project_type, title_text),
                                        'status': 'new'
                                    }
                                    
                                    self.leads_found.append(lead)
                                    print(f"  ‚úÖ Lead trouv√©: {title_text[:50]}...")
                    
                    time.sleep(2)
                
                except Exception as e:
                    print(f"  ‚ö†Ô∏è Erreur requ√™te: {e}")
                    continue
        
        except Exception as e:
            print(f"  ‚ùå Erreur scraping Google News: {e}")

    def scrape_marches_publics(self, department):
        """Scrape les march√©s publics pour les appels d'offres"""
        print(f"üîç Scraping March√©s Publics - D√©partement {department}...")
        
        try:
            # Simulation de donn√©es (√† remplacer par vrai scraping de BOAMP)
            # En production, scraper : https://www.boamp.fr
            
            mock_projects = [
                {
                    'name': f'Construction zone commerciale - D√©partement {department}',
                    'type': 'Centre Commercial',
                    'department': department,
                    'location': f'{CITIES[department][0]}, {department}',
                    'source': 'BOAMP - Appel d\'offres public',
                    'notes': 'Appel d\'offres pour marquage au sol zone commerciale',
                    'estimated_value': '25 000 - 40 000 ‚Ç¨',
                    'parking_size': '200+ places',
                    'status': 'new'
                }
            ]
            
            # En production, impl√©menter vrai scraping ici
            print(f"  ‚ÑπÔ∏è March√©s publics : √† impl√©menter avec acc√®s API BOAMP")
        
        except Exception as e:
            print(f"  ‚ùå Erreur scraping March√©s Publics: {e}")

    def add_leads_to_supabase(self):
        """Ajoute les leads trouv√©s dans Supabase"""
        print(f"\nüíæ Ajout de {len(self.leads_found)} leads dans Supabase...")
        
        # Note: Pour la version automatique, il faudrait un user_id syst√®me
        # Pour l'instant, on va logger les leads trouv√©s
        
        for lead in self.leads_found:
            try:
                # En production, ajouter √† Supabase avec user_id appropri√©
                print(f"  ‚úÖ Lead pr√™t: {lead['name'][:50]}... ({lead['location']})")
                
                # Pour l'instant, juste afficher
                # En production, d√©commenter:
                # response = supabase.table('leads').insert(lead).execute()
                
            except Exception as e:
                print(f"  ‚ùå Erreur ajout lead: {e}")

    def run(self):
        """Lance le scraping complet"""
        print("="*80)
        print("üöÄ PROGMARQUAGE - SYST√àME DE SCRAPING AUTOMATIQUE")
        print("="*80)
        print(f"üìÖ Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"üéØ R√©gions: Savoie (73), Haute-Savoie (74), Ain (01)")
        print("="*80)
        
        # Scraper chaque d√©partement et ville
        for dept in DEPARTMENTS:
            print(f"\nüìç D√âPARTEMENT {dept}")
            print("-"*80)
            
            for city in CITIES[dept][:3]:  # Limiter √† 3 villes par d√©partement pour test
                self.scrape_dauphine_libere(dept, city)
                time.sleep(3)
                
                self.scrape_google_news(dept, city)
                time.sleep(3)
            
            self.scrape_marches_publics(dept)
            time.sleep(3)
        
        # Afficher r√©sum√©
        print("\n" + "="*80)
        print(f"‚úÖ SCRAPING TERMIN√â - {len(self.leads_found)} LEADS TROUV√âS")
        print("="*80)
        
        # Sauvegarder les leads
        if self.leads_found:
            self.save_leads_to_json()
            # self.add_leads_to_supabase()  # D√©commenter en production
        else:
            print("‚ÑπÔ∏è Aucun nouveau lead d√©tect√©")

    def save_leads_to_json(self):
        """Sauvegarde les leads dans un fichier JSON"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f"leads_progmarquage_{timestamp}.json"
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(self.leads_found, f, ensure_ascii=False, indent=2)
        
        print(f"\nüíæ Leads sauvegard√©s dans: {filename}")
        
        # Afficher aper√ßu
        print("\nüìã APER√áU DES LEADS TROUV√âS:")
        print("-"*80)
        for i, lead in enumerate(self.leads_found[:5], 1):
            print(f"{i}. {lead['name'][:60]}")
            print(f"   üìç {lead['location']} | üí∞ {lead['estimated_value']}")
            print(f"   üÖøÔ∏è {lead['parking_size']} | üì∞ {lead['source']}")
            print()

if __name__ == "__main__":
    scraper = ProgMarquageScraper()
    scraper.run()
